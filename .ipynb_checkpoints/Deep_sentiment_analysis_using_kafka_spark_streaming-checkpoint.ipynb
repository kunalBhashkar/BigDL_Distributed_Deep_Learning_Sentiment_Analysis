{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunal/anaconda3/envs/deep_learning/lib/python3.6/site-packages/bigdl/util/engine.py:41: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /home/kunal/Downloads/spark-2.4.7-bin-hadoop2.7/, and pyspark is found in: /home/kunal/anaconda3/envs/deep_learning/lib/python3.6/site-packages/pyspark/__init__.py. If they are unmatched, please use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepending /home/kunal/anaconda3/envs/deep_learning/lib/python3.6/site-packages/bigdl/share/conf/spark-bigdl.conf to sys.path\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import itertools\n",
    "import re\n",
    "import os\n",
    "from optparse import OptionParser\n",
    "from bigdl.dataset import news20\n",
    "from bigdl.util.common import Sample\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import datetime as dt\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml import  Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "from bigdl.dataset.transformer import *\n",
    "from bigdl.dataset.base import *\n",
    "from bigdl.nn.layer import *\n",
    "from bigdl.nn.criterion import *\n",
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.util.common import *\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the path for Spark\n",
    "os.environ['JAVA_HOME']=\"/usr/lib/jvm/java-8-oracle\"\n",
    "os.environ['SPARK_HOME'] = \"/home/kunal/Downloads/spark-2.4.7-bin-hadoop2.7\"\n",
    "os.environ['HADOOP_HOME'] = \"/usr/local/hadoop/bin\"\n",
    "os.environ['YARN_CONF_DIR'] = \"/usr/local/hadoop/etc/hadoop\"\n",
    "os.environ['BIGDL_HOME'] = \"/home/kunal/anaconda3/envs/deep_learning/lib/python3.6/site-packages/bigdl/share/bin\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /home/kunal/Downloads/jarfiles/bigdl-SPARK_2.2-0.7.0-jar-with-dependencies.jar --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.7,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.7,org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Spark Session\n",
    "sc=SparkContext.getOrCreate(conf=create_spark_conf().setMaster(\"local[4]\").set(\"spark.driver.memory\",\"8g\"))\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "init_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to get train data\n",
    "def get_train_data(train_file):\n",
    "    texts = []\n",
    "    nlines = 0\n",
    "    with open(train_file) as f:\n",
    "        for line in f:\n",
    "            text_label = re.split(r'\\t+', line)\n",
    "            text_label[0] = int(text_label[0])+1\n",
    "            texts.append((text_label[1],text_label[0]))\n",
    "            nlines = nlines+1\n",
    "    return (texts,nlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to get test data\n",
    "def get_test_data(test_file):\n",
    "    texts = []\n",
    "    nlines = 0\n",
    "    with open(test_file) as f:\n",
    "        for line in f:\n",
    "            texts.append((line,1))\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to convert text to words\n",
    "def text_to_words(review_text):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    words = letters_only.lower().split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the frequency of words in each text corpus, \n",
    "# sort by frequency (max to min)\n",
    "# and assign an id to each word\n",
    "def analyze_texts(data_rdd):\n",
    "    def index(w_c_i):\n",
    "        ((w, c), i) = w_c_i\n",
    "        return (w, (i + 1, c))\n",
    "    return data_rdd.flatMap(lambda text_label: text_to_words(text_label[0])) \\\n",
    "        .map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b) \\\n",
    "        .sortBy(lambda w_c: - w_c[1]).zipWithIndex() \\\n",
    "        .map(lambda w_c_i: index(w_c_i)).collect()  #return a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method for padding\n",
    "# pad([1, 2, 3, 4, 5], 0, 6)\n",
    "def pad(l, fill_value, width):\n",
    "    if len(l) >= width:\n",
    "        return l[0: width]\n",
    "    else:\n",
    "        l.extend([fill_value] * (width - len(l)))\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to vector\n",
    "def to_vec(token, b_w2v, embedding_dim):\n",
    "    if token in b_w2v:\n",
    "        return b_w2v[token]\n",
    "    else:\n",
    "        return pad([], 0, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to Sample for BigDL prediction\n",
    "def to_sample(vectors, label, embedding_dim):\n",
    "    # flatten nested list\n",
    "    flatten_features = list(itertools.chain(*vectors))\n",
    "    # a row for each word vector\n",
    "    features = np.array(flatten_features, dtype='float').reshape([sequence_len, embedding_dim])\n",
    "    features = features.transpose(1, 0)\n",
    "    return Sample.from_ndarray(features, np.array(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to build Convolution neural network\n",
    "def build_cnn(class_num, input_dim, hidden_dim):\n",
    "    #each row is an input vector for the RNN\n",
    "    model = Sequential()\n",
    "    model.add(Reshape([input_dim, 1, sequence_len]))\n",
    "    model.add(SpatialConvolution(input_dim, hidden_dim, 5, 1))\n",
    "    model.add(ReLU())\n",
    "    model.add(SpatialMaxPooling(5, 1, 5, 1))\n",
    "    model.add(SpatialConvolution(hidden_dim, hidden_dim, 5, 1))\n",
    "    model.add(ReLU())\n",
    "    model.add(SpatialMaxPooling(5, 1, 5, 1))\n",
    "    model.add(Reshape([hidden_dim]))\n",
    "    model.add(Linear(hidden_dim, 100))\n",
    "    model.add(Linear(100, class_num))\n",
    "    model.add(LogSoftMax())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to process text\n",
    "def preprocess_texts(data_rdd, sequence_len, max_words, embedding_dim, streaming):\n",
    "    #get list of (word, (index,freq)) representing the training set\n",
    "    word_to_ic = analyze_texts(data_rdd)\n",
    "\n",
    "    #at most max_words words among the most frequent\n",
    "    if streaming:\n",
    "        word_to_ic = dict(word_to_ic)\n",
    "    else:\n",
    "        word_to_ic = dict(word_to_ic[10: max_words])\n",
    "    \n",
    "    bword_to_ic = sc.broadcast(word_to_ic)\n",
    "\n",
    "    #prepare and broadcast word embeddings filtered through word_to_ic \n",
    "    w2v = news20.get_glove_w2v(dim=embedding_dim)\n",
    "    filtered_w2v = {w: v for w, v in w2v.items() if w in word_to_ic}\n",
    "    bfiltered_w2v = sc.broadcast(filtered_w2v)\n",
    "\n",
    "    #get a list of words for each line + label in data_rdd\n",
    "    tokens_rdd = data_rdd.map(lambda text_label:\n",
    "                              ([w for w in text_to_words(text_label[0]) if\n",
    "                                w in bword_to_ic.value], text_label[1]))\n",
    "\n",
    "    #pad lists of words to sequence_len size + label\n",
    "    padded_tokens_rdd = tokens_rdd.map( lambda tokens_label: \n",
    "                                        (pad(tokens_label[0], \"##\", sequence_len), tokens_label[1]))\n",
    "\n",
    "    #get vectors from words + label\n",
    "    vector_rdd = padded_tokens_rdd.map(lambda tokens_label:\n",
    "                                       ([to_vec(w, bfiltered_w2v.value,\n",
    "                                                embedding_dim) for w in\n",
    "                                         tokens_label[0]], tokens_label[1]))\n",
    "\n",
    "    #get matrix sample composed by word vectors for each text\n",
    "    sample_rdd = vector_rdd.map(\n",
    "        lambda vectors_label: to_sample(vectors_label[0], vectors_label[1], embedding_dim))\n",
    "\n",
    "    return sample_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to Map, classify and show\n",
    "def map_predict_label(l):\n",
    "    return np.array(l).argmax()\n",
    "\n",
    "def classify_stream(rdd_test, train_model):\n",
    "    if not(rdd_test.isEmpty()):\n",
    "        #probability vectors, one for each input\n",
    "        predictions = train_model.predict(rdd_test).collect()\n",
    "        #get max probability indices\n",
    "        y_pred = np.array([ map_predict_label(s) for s in predictions])\n",
    "        for y in y_pred:\n",
    "            if y==0: \n",
    "                print('NEGATIVE\\n')\n",
    "            else:\n",
    "                print('POSITIVE\\n')\n",
    "\n",
    "def show(rdd_test):\n",
    "    for elem in rdd_test.collect():\n",
    "        print(elem[0].encode('utf-8'),end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "embedding_dim = 50\n",
    "model_path = \"model_path\"\n",
    "app_name = \"stream_classifier\"\n",
    "sequence_len = 50   #number of words for each text\n",
    "max_words = 1000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training data...\n",
      "train data loaded!\n",
      "number of lines for training set: 7086\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "batch_size = 128\n",
    "max_epoch = 100\n",
    "#p = float(options.p)\n",
    "checkpoint_path = \"checkpoint\"\n",
    "logdir = \"log\"\n",
    "train_file = \"./data/train.txt\"\n",
    "training_split = 0.8\n",
    "class_num = 2 \n",
    "\n",
    "print('loading training data...')\n",
    "#get training data\n",
    "(train_data,nlines) = get_train_data(train_file)\n",
    "#train_data = get_data('./acllmdb','train')\n",
    "print('train data loaded!')\n",
    "print('number of lines for training set: ' + str(nlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "#Train and Optimize data\n",
    "print('Processing text dataset')\n",
    "#generate RDD and sample RDD for the optimizer\n",
    "train_data_rdd = sc.parallelize(train_data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The Da Vinci Code book is just awesome.\\n', 2),\n",
       " (\"this was the first clive cussler i've ever read, but even books like Relic, and Da Vinci code were more plausible than this.\\n\",\n",
       "  2),\n",
       " ('i liked the Da Vinci Code a lot.\\n', 2),\n",
       " ('i liked the Da Vinci Code a lot.\\n', 2),\n",
       " (\"I liked the Da Vinci Code but it ultimatly didn't seem to hold it's own.\\n\",\n",
       "  2),\n",
       " (\"that's not even an exaggeration ) and at midnight we went to Wal-Mart to buy the Da Vinci Code, which is amazing of course.\\n\",\n",
       "  2),\n",
       " ('I loved the Da Vinci Code, but now I want something better and different!..\\n',\n",
       "  2),\n",
       " ('i thought da vinci code was great, same with kite runner.\\n', 2),\n",
       " ('The Da Vinci Code is actually a good movie...\\n', 2),\n",
       " ('I thought the Da Vinci Code was a pretty good book.\\n', 2)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take top 10\n",
    "train_data_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rdd = preprocess_texts(train_data_rdd, sequence_len, max_words, embedding_dim, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createSequential\n",
      "creating: createReshape\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createReshape\n",
      "creating: createLinear\n",
      "creating: createLinear\n",
      "creating: createLogSoftMax\n",
      "creating: createClassNLLCriterion\n",
      "creating: createMaxEpoch\n",
      "creating: createAdagrad\n",
      "creating: createDistriOptimizer\n",
      "creating: createEveryEpoch\n",
      "creating: createTop1Accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunal/anaconda3/envs/deep_learning/lib/python3.6/site-packages/bigdl/optim/optimizer.py:864: UserWarning: You are recommended to use `create` method to create an optimizer.\n",
      "  warnings.warn(\"You are recommended to use `create` method to create an optimizer.\")\n"
     ]
    }
   ],
   "source": [
    "#split into training and validation set\n",
    "train_rdd, val_rdd = sample_rdd.randomSplit([training_split, 1-training_split])\n",
    "\n",
    "optimizer = Optimizer(\n",
    "    model=build_cnn(class_num, input_dim=embedding_dim, hidden_dim=128),\n",
    "    training_rdd=train_rdd,\n",
    "    criterion=ClassNLLCriterion(),\n",
    "    end_trigger=MaxEpoch(max_epoch),\n",
    "    batch_size=batch_size,\n",
    "    optim_method=Adagrad(learningrate=0.01, learningrate_decay=0.0002))\n",
    "\n",
    "optimizer.set_validation(\n",
    "    batch_size=batch_size,\n",
    "    val_rdd=val_rdd,\n",
    "    trigger=EveryEpoch(),\n",
    "    val_method=[Top1Accuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createEveryEpoch\n",
      "creating: createTrainSummary\n",
      "creating: createValidationSummary\n"
     ]
    }
   ],
   "source": [
    "#set checkpoint path for model\n",
    "optimizer.set_checkpoint(EveryEpoch(), checkpoint_path)\n",
    "#set train and val log for tensorboard\n",
    "train_summary = TrainSummary(log_dir=logdir, app_name=app_name)\n",
    "val_summary = ValidationSummary(log_dir=logdir, app_name=app_name)\n",
    "optimizer.set_train_summary(train_summary)\n",
    "optimizer.set_val_summary(val_summary)\n",
    "\n",
    "train_model=optimizer.optimize()\n",
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = get_test_data(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_rdd = sc.parallelize(test_data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rdd = preprocess_texts(test_data_rdd, sequence_len, max_words, embedding_dim, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the trained model from the model path\n",
    "#train_model = Model.load(model_path)\n",
    "predictions = train_model.predict(sample_rdd).take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" I don't care what anyone says, I like Hillary Clinton.\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "how i destroy the earth...\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "have an awesome time at purdue!..\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "Yep, I'm still in London, which is pretty awesome: P Remind me to post the million and one pictures that I took when I get back to Markham!...\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "Have to say, I hate Paris Hilton's behavior but I do think she's kinda cute..\n",
      " (NEGATIVE)\n",
      "\n",
      "\n",
      "i will love the lakers.\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "deepak jha is good boy.\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "Kunal Bhashkar is Happy.\n",
      " (NEGATIVE)\n",
      "\n",
      "\n",
      "Kunal Bhashkar is good boy.\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "deepak jha is bad boy.\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "I'm so glad I love Paris Hilton, too, or this would be excruciating.\n",
      " (NEGATIVE)\n",
      "\n",
      "\n",
      "considering most Geico commericals are stupid...\n",
      " (NEGATIVE)\n",
      "\n",
      "\n",
      "i liked MIT though, esp their little info book(\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "Before I left Missouri, I thought London was going to be so good and cool and fun and a really great experience and I was really excited.\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "I still like Tom Cruise.\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "well, i had a piece of crap toyota celica but it died in portland and i got a ford ranger..\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "i love angelina jolie.\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "I still like Tom Cruise.\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "UCLA is beautiful.\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "I think Angelina Jolie is so much more beautiful than Jennifer Anniston, who, by the way, is majorly OVERRATED.\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "Angelina Jolie is beautiful.\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "and honda's are awesome:).\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "I love Harvard.\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "i love tom cruise!..\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "This means we beat out schools like MIT, which is amazing for a relatively small, unassuming lil'IS department.\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "i hate london bugs.\n",
      " (NEGATIVE)\n",
      "\n",
      "\n",
      "Way to go stupid Lakers..\n",
      " (NEGATIVE)\n",
      "\n",
      "\n",
      "london sucks....\n",
      " (NEGATIVE)\n",
      "\n",
      "\n",
      "anyway, shanghai is really beautiful ï¼Œ æ»¨æ±Ÿå¤§é“æ˜¯å¾ˆçµçš„å•¦ ï¼Œ é‚£ä¸ªstarbucksæ˜¯ä¸Šæµ·é£Žæ™¯æœ€å¥½çš„starbucks ~ ~!!!\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "buy quite a few food to back to notts to eat la, aiiii, notts only hv 1 chinese shop in town ja, so shit, london is so GREAT!!..\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "angelina jolie is so beautiful that i don't even have the desire to attain such exquisite beauty..\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "I reallllllly hate Tom Cruise...\n",
      " (NEGATIVE)\n",
      "\n",
      "\n",
      "To my understanding, Harvard is a very difficult college to get in to.\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "Boston can suck my fucking tits...\n",
      " (NEGATIVE)\n",
      "\n",
      "\n",
      "I loved Boston and MIT so much and still do.\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "I like honda civics!!!!!!.\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "no matter how obvious it is to me that George W Bush is an arrogant idiot liar, there are plenty of people who believe him.\n",
      " (NEGATIVE)\n",
      "\n",
      "\n",
      "I love the Los Angeles Lakers...\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "the stupid honda lol or a BUG!..\n",
      " (NEGATIVE)\n",
      "\n",
      "\n",
      "seattle sucks anyways.\n",
      " (NEGATIVE)\n",
      "\n",
      "\n",
      "I want a ThinkPad or something.\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "I love Shanghai, it's such a great city, and Hongzhou is only a two hour train ride away from it.\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "My Purdue Cal friends are awesome!..\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "Shanghai is beautiful ~..\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "london sucks....\n",
      " (NEGATIVE)\n",
      "\n",
      "\n",
      "i love my new Macbook..\n",
      " (NEGATIVE)\n",
      "\n",
      "\n",
      "i love my new Macbook..\n",
      " (NEGATIVE)\n",
      "\n",
      "\n",
      "i'd love to see the clips and lakers in the second round, though the winner would just be a stepping stone for the mavs or spurs...\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "Awesome diner here @ Purdue...\n",
      " (POSITIVE)\n",
      "\n",
      "\n",
      "i love my new Macbook..\n",
      " (NEGATIVE)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get max probability indices\n",
    "y_pred = np.array([ map_predict_label(s) for s in predictions])\n",
    "i = 0\n",
    "for y in y_pred:\n",
    "    print(test_data[i][0], end=' ')\n",
    "    if y==0: \n",
    "        print('(NEGATIVE)')\n",
    "    else:\n",
    "        print('(POSITIVE)')\n",
    "    print('\\n')\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction from Kafka Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating Streaming context\n",
    "ssc = StreamingContext(sc, 5)\n",
    "topic = \"sentiment\"  #kafka topic\n",
    "zkQuorum = 'localhost:2181' #zk server\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /home/kunal/Downloads/jarfiles/spark-streaming-kafka-0-8-assembly_2.11-2.4.7.jar --packages org.apache.spark:spark-streaming-kafka-0-8:2.4.7 pyspark-shell'\n",
    "#get the trained model from the model path\n",
    "#train_model = Model.load(model_path)\n",
    "\n",
    "#generate and handle stream\n",
    "kafkastream = KafkaUtils.createStream(ssc, zkQuorum, \"spark-streaming-consumer\", {topic: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\" I don\\'t care what anyone says, I like Hillary Clinton.\\n'POSITIVE\n",
      "\n",
      "b'how i destroy the earth...\\n'POSITIVE\n",
      "\n",
      "b'have an awesome time at purdue!..\\n'POSITIVE\n",
      "\n",
      "b\"Yep, I'm still in London, which is pretty awesome: P Remind me to post the million and one pictures that I took when I get back to Markham!...\\n\"POSITIVE\n",
      "\n",
      "b\"Have to say, I hate Paris Hilton's behavior but I do think she's kinda cute..\\n\"NEGATIVE\n",
      "\n",
      "b'i will love the lakers.\\n'POSITIVE\n",
      "\n",
      "b'deepak jha is good boy.\\n'POSITIVE\n",
      "\n",
      "b'Kunal Bhashkar is Happy.\\n'POSITIVE\n",
      "\n",
      "b'Kunal Bhashkar is good boy.\\n'POSITIVE\n",
      "\n",
      "b'deepak jha is bad boy.\\n'NEGATIVE\n",
      "\n",
      "b\"I'm so glad I love Paris Hilton, too, or this would be excruciating.\\n\"POSITIVE\n",
      "\n",
      "b'considering most Geico commericals are stupid...\\n'"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-a304dcf9dddc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/site-packages/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \"\"\"\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Read and predict data from Kafka streaming\n",
    "ks = kafkastream.map(lambda ks: (ks[1],0)) #get lines with fake classes\n",
    "ks.foreachRDD(show) #show lines\n",
    "featstream = ks.transform(lambda rdd: \n",
    "                            preprocess_texts(rdd, sequence_len, max_words, embedding_dim, True)) #get w2v\n",
    "featstream.foreachRDD(lambda fs: classify_stream(fs,train_model)) #classify stream\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
